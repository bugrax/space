# ğŸ¦ Twitter SaaS Idea Finder

Twitter'dan (X) validated SaaS fikirlerini bulan, analiz eden ve skorlayan bir microservice.

## âœ¨ Features

- ğŸ” Twitter'dan MRR/ARR/Revenue iÃ§eren tweet'leri toplama
- ğŸ’° MRR/ARR/Revenue rakamlarÄ±nÄ± otomatik parse etme
- ğŸ”— ÃœrÃ¼n URL'lerini Ã§Ä±karma ve doÄŸrulama
- ğŸ“Š 4 filtreli scoring sistemi (Traction, Growth, Traffic, Simplicity)
- ğŸ’¾ PostgreSQL ile persistent storage
- ğŸŒ FastAPI REST API + Next.js Dashboard
- â° Celery ile scheduled scraping (30 dakikada bir)
- ğŸ³ Docker Compose ile tam stack deployment
- ğŸŒ¸ Flower ile Celery monitoring

## ğŸ“ Project Structure

```
â”œâ”€â”€ saas_finder/           # Main Python package
â”‚   â”œâ”€â”€ api.py             # FastAPI application
â”‚   â”œâ”€â”€ celery_app.py      # Celery configuration & schedules
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ finder.py          # Main finder logic
â”‚   â”œâ”€â”€ tasks.py           # Celery tasks
â”‚   â”œâ”€â”€ extractors/        # Revenue & URL extraction
â”‚   â”œâ”€â”€ parsers/           # MRR/URL parsing
â”‚   â”œâ”€â”€ scoring/           # Idea scoring system
â”‚   â”œâ”€â”€ storage/           # Database models & operations
â”‚   â””â”€â”€ twitter/           # Twitter scraping clients
â”‚       â””â”€â”€ scrapers/      # Apify, twscrape clients
â”œâ”€â”€ config/                # Configuration files
â”‚   â”œâ”€â”€ accounts.txt       # Twitter credentials (gitignored)
â”‚   â””â”€â”€ accounts.txt.example
â”œâ”€â”€ docker/                # Docker configuration
â”‚   â””â”€â”€ Dockerfile         # Python services image
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ analyze_tweets.py  # CLI for tweet analysis
â”‚   â”œâ”€â”€ seed_data.py       # Seed sample data
â”‚   â””â”€â”€ migrate_to_postgres.py
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ web/                   # Next.js frontend dashboard
â”‚   â”œâ”€â”€ Dockerfile         # Next.js standalone build
â”‚   â””â”€â”€ src/app/           # React components
â”œâ”€â”€ data/                  # Data output directory
â”œâ”€â”€ docker-compose.yml     # Full stack orchestration
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸ›  Tech Stack

| Component       | Technology                    |
| --------------- | ----------------------------- |
| Language        | Python 3.11+                  |
| Primary Scraper | Apify (apidojo/tweet-scraper) |
| Database        | PostgreSQL 16                 |
| Cache/Broker    | Redis 7                       |
| Task Queue      | Celery                        |
| API             | FastAPI                       |
| Frontend        | Next.js 14                    |
| Monitoring      | Flower                        |
| Container       | Docker Compose                |

## ğŸš€ Quick Start with Docker

```bash
# 1. Clone repository
git clone <repo>
cd space

# 2. Environment variables
cp .env.example .env
# Edit .env and add APIFY_API_TOKEN

# 3. Start all services (7 containers)
docker compose up -d

# 4. Check status
docker compose ps

# 5. View API logs
docker compose logs -f api
```

### Services & Ports

| Service       | Port | Description               |
| ------------- | ---- | ------------------------- |
| postgres      | 5432 | PostgreSQL database       |
| redis         | 6379 | Message broker & cache    |
| api           | 8000 | FastAPI backend           |
| web           | 3000 | Next.js dashboard         |
| flower        | 5555 | Celery monitoring UI      |
| celery_worker | -    | Background task processor |
| celery_beat   | -    | Scheduled task runner     |

## ğŸ”‘ API Keys Setup

### Apify (Required)

1. Sign up at [apify.com](https://apify.com)
2. Subscribe to [apidojo/tweet-scraper](https://apify.com/apidojo/tweet-scraper)
3. Get your API token from Settings
4. Add to `.env`:

```
APIFY_API_TOKEN=apify_api_xxx
```

## ğŸ“¡ API Endpoints

```bash
# Health check
curl http://localhost:8000/health

# Scraper status
curl http://localhost:8000/api/scraper/status

# List all ideas
curl "http://localhost:8000/api/ideas?limit=10"

# Dashboard stats
curl http://localhost:8000/api/stats

# Manual scrape with specific queries
curl -X POST http://localhost:8000/api/scraper/search \
  -H "Content-Type: application/json" \
  -d '{"queries": ["\"$5k MRR\"", "\"$10k MRR\""], "limit_per_query": 20, "min_mrr": 0}'

# Trigger background scrape
curl -X POST http://localhost:8000/api/scraper/trigger
```

## â° Scheduled Tasks

| Task                | Schedule          | Description                         |
| ------------------- | ----------------- | ----------------------------------- |
| scan_revenue_tweets | Every 30 min      | Scans for MRR/ARR tweets            |
| scan_hashtags       | Every hour        | Scans #buildinpublic, #indiehackers |
| deep_scan           | Daily 3 AM        | Deep scan across all queries        |
| rescore_ideas       | Every 6 hours     | Updates idea scores                 |
| cleanup_old_data    | Weekly (Sun 4 AM) | Cleans old data                     |

## ğŸ“Š Revenue Detection Patterns

The system automatically detects:

- `$10,000 MRR` / `$10K MRR`
- `$50,000 ARR` / `$50K ARR`
- `$5,000/month` / `$5K per month`
- `"hit $10K"` / `"crossed $50,000"`
- `"5 figure MRR"` / `"6 figure revenue"`
- Stripe screenshot indicators

## ğŸ§ª Running Tests

```bash
# Using Docker
docker compose exec api pytest tests/ -v

# Or locally
pytest tests/ -v --cov=saas_finder
```

## ğŸ“‹ VS Code Tasks

The project includes pre-configured VS Code tasks:

- **Start Docker Services** - `docker compose up -d`
- **Start API Server** - Local uvicorn server
- **Run Tests** - pytest with verbose output
- **Analyze Tweets** - Run tweet analysis script
- **Seed Sample Data** - Populate database with samples
- **Check Scraper Status** - Query API for scraper health

## ğŸ”§ Environment Variables

```bash
# Required
APIFY_API_TOKEN=apify_api_xxx

# Database (defaults work with Docker)
DATABASE_URL=postgresql://saas_finder:saas_finder@postgres:5432/saas_finder
REDIS_URL=redis://redis:6379/0

# Optional
MIN_MRR_THRESHOLD=500
MAX_TWEETS_PER_QUERY=100
SCRAPE_INTERVAL_HOURS=4
```

## ğŸ“ License

MIT
